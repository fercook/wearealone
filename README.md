# wearealone
Chatting with a neural network

seq2seq para Keras: 

* https://github.com/farizrahman4u/seq2seq
* https://github.com/nicolas-ivanov/debug_seq2seq

Pretrained word vectors mas grandes que word2vec:
* https://nlp.stanford.edu/projects/glove/
** adem√°s tiene un dataset con embeddings de 25 y 50 dimensiones
** https://nlp.stanford.edu/data/

Como usarlos en Keras (en general): https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html
